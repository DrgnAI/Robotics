{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e40e8ebc-f912-4c3b-8cc8-936956146a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_line(frame):\n",
    "    # Define the range for yellow and blue colors in HSV\n",
    "    color_ranges = {\n",
    "        'yellow': ((20, 100, 100), (30, 255, 255)),\n",
    "        'blue': ((85, 50, 50), (135, 255, 255))\n",
    "    }\n",
    "\n",
    "    # Convert frame to HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Prepare to collect regions of interest\n",
    "    regions = []\n",
    "\n",
    "    # Process each color\n",
    "    for color, (lower, upper) in color_ranges.items():\n",
    "        # Create mask and dilate\n",
    "        mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\n",
    "\n",
    "        # Check if the current color is yellow\n",
    "        if color == 'yellow':\n",
    "            mask = cv2.dilate(mask, np.ones((5, 5), np.uint8), iterations=5)\n",
    "\n",
    "\n",
    "        # Apply mask\n",
    "        extracted_region = cv2.bitwise_and(hsv, hsv, mask=mask)\n",
    "        regions.append(extracted_region)\n",
    "        \n",
    "    # Combine all the regions of interest\n",
    "    combined_region = cv2.bitwise_or(regions[0], regions[1])\n",
    "\n",
    "    # Convert all detected regions to white\n",
    "    combined_region[combined_region > 0] = 255\n",
    "\n",
    "    return combined_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a47a5eeb-4fa2-4e99-94ad-605d3cf83606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_line_contours(edges, frame):\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "     # Process contours\n",
    "    if contours:\n",
    "        line_contours = []\n",
    "        for contour in contours:\n",
    "            if cv2.contourArea(contour) > 1500:\n",
    "                line_contours.append(contour)\n",
    "                cv2.drawContours(frame, [contour], -1, (0, 255, 0), cv2.FILLED)\n",
    "            else:\n",
    "                cv2.drawContours(frame, [contour], -1, (0, 0, 255), cv2.FILLED)\n",
    "        return line_contours, frame\n",
    "    else:\n",
    "        return [], frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53d90055-60e1-4359-bb4f-1d2d64774a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contours_with_centroid(contours, frame):\n",
    "    center_of_frame = (frame.shape[1] // 2, frame.shape[0] // 2)\n",
    "    cv2.circle(frame, center_of_frame, 10, (0, 0, 0), cv2.FILLED)\n",
    "\n",
    "    def calculate_contour_centroid(contour):\n",
    "        moments = cv2.moments(contour)\n",
    "        if moments['m00'] != 0:\n",
    "            centroid_x = int(moments['m10'] / moments['m00'])\n",
    "            centroid_y = int(moments['m01'] / moments['m00'])\n",
    "            return (centroid_x, centroid_y)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # List of contours with their centroids\n",
    "    contours_with_centroids = [(contour, calculate_contour_centroid(contour)) for contour in contours]\n",
    "    # Filter out any contours without a valid centroid\n",
    "    valid_contours_with_centroids = [contour_data for contour_data in contours_with_centroids if contour_data[1] is not None]\n",
    "    \n",
    "    if valid_contours_with_centroids:\n",
    "        # Find the contour closest to the center of the frame based on its area\n",
    "        largest_contour_data = max(valid_contours_with_centroids, key=lambda item: cv2.contourArea(item[0]))\n",
    "        largest_contour_centroid = largest_contour_data[1]\n",
    "\n",
    "        if largest_contour_centroid:\n",
    "            cv2.circle(frame, largest_contour_centroid, 12, (0, 0, 0), cv2.FILLED)\n",
    "            cv2.line(frame, largest_contour_centroid, center_of_frame, (255, 255, 255), 8)\n",
    "\n",
    "    return valid_contours_with_centroids, largest_contour_centroid, largest_contour_data[0], frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ded066a-0f75-4fa2-ad1d-f8083762434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_turn_boundaries(image, contours_info):\n",
    "    # Boundary definitions\n",
    "    sharp_turn_left_threshold = round(image.shape[1] * 0.22)\n",
    "    sharp_turn_right_threshold = image.shape[1] - sharp_turn_left_threshold\n",
    "    lower_image_boundary = image.shape[0] - round(image.shape[0] * 0.15)\n",
    "\n",
    "    # Initialize lists for out-of-bound contours\n",
    "    left_out_contours = []\n",
    "    right_out_contours = []\n",
    "\n",
    "    # Determine contour positions relative to boundaries\n",
    "    for contour, _ in contours_info:\n",
    "        contour_points = np.squeeze(contour)\n",
    "        if np.any(contour_points[:, 0] < sharp_turn_left_threshold) and np.any(contour_points[:, 1] > lower_image_boundary):\n",
    "            left_out_contours.append(contour)\n",
    "        elif np.any(contour_points[:, 0] > sharp_turn_right_threshold) and np.any(contour_points[:, 1] > lower_image_boundary):\n",
    "            right_out_contours.append(contour)\n",
    "\n",
    "    return left_out_contours, right_out_contours, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1de7b088-540f-4f5c-89df-0bef948016e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyrealsense2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#using realsense to capture the color and depth image\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyrealsense2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrs\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#multi-threading is used to capture the image in real time performance\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyrealsense2'"
     ]
    }
   ],
   "source": [
    "#use traitlets and widgets to display the image in Jupyter Notebook\n",
    "import traitlets\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "#use opencv to covert the depth image to RGB image for displaying purpose\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#using realsense to capture the color and depth image\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "#multi-threading is used to capture the image in real time performance\n",
    "import threading\n",
    "import time\n",
    "# run this code on the robot, make sure you change the IP address(line 14) to the target computer's address\n",
    "import random\n",
    "import socket, select\n",
    "from time import gmtime, strftime\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "class Camera(SingletonConfigurable):\n",
    "    \n",
    "    #this changing of this value will be captured by traitlets\n",
    "    color_value = traitlets.Any()\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "        \n",
    "        #configure the color and depth sensor\n",
    "        self.pipeline = rs.pipeline()\n",
    "        self.configuration = rs.config()  \n",
    "        \n",
    "        #set resolution for the color camera\n",
    "        self.color_width = 1280\n",
    "        self.color_height = 720\n",
    "        self.color_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.color, self.color_width, self.color_height, rs.format.bgr8, self.color_fps)\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "        \n",
    "        #start the RGBD sensor\n",
    "        self.pipeline.start(self.configuration)\n",
    "        self.pipeline_started = True\n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "\n",
    "        #start capture the first color image\n",
    "        color_frame = frames.get_color_frame()   \n",
    "        image = np.asanyarray(color_frame.get_data())\n",
    "        self.color_value = image \n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            frames = self.pipeline.wait_for_frames() #receive data from RGBD sensor\n",
    "            color_frame = frames.get_color_frame() #get the color image\n",
    "            image = np.asanyarray(color_frame.get_data()) #convert color image to numpy array\n",
    "            # Set top 605 pixels to zero\n",
    "            image[:605,:] = 0\n",
    "        \n",
    "            # Set bottom pixels; none in this case as the slicing retains the lower part\n",
    "            \n",
    "            # Set the first 150 pixels on the left to zero\n",
    "            image[:,:150] = 0\n",
    "        \n",
    "            # Set the pixels after 1130 on the right to zero\n",
    "            image[:,1130:] = 0\n",
    "            self.color_value = image #assign the numpy array image to the color_value variable             \n",
    "    \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread       \n",
    "\n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera.instance()\n",
    "camera.start() # start capturing the data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b12e7b3-8716-4bf0-8b4c-8ff51fca3e4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipywidgets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwidgets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mwidgets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, HTML\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "import datetime\n",
    "import tensorrt as trt\n",
    "import time\n",
    "from RobotClass import Robot\n",
    "\n",
    "robot = Robot()\n",
    "previous_direction = \"forward\"\n",
    "speed = 0.8\n",
    "#create widgets for the displaying of the image\n",
    "display_color = widgets.Image(format='jpeg', width='45%') #determine the width of the color image\n",
    "display_depth = widgets.Image(format='jpeg', width='45%')  #determine the width of the depth image\n",
    "layout=widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth],layout=layout) #horizontal \n",
    "display(sidebyside) #display the widget\n",
    "\n",
    "#callback function, invoked when traitlets detects the changing of the color image\n",
    "def process(change):\n",
    "    global previous_direction\n",
    "    image = change['new']\n",
    "    line = detect_line(image)\n",
    "    line_contours, image = find_line_contours(line[:, :, 0], image)\n",
    "\n",
    "    if line_contours:\n",
    "        valid_contours_with_centroids, largest_contour_centroid, largest_contour, image = contours_with_centroid(line_contours, image)\n",
    "        left_out_contours, right_out_contours, image = evaluate_turn_boundaries(image, valid_contours_with_centroids)\n",
    "\n",
    "        sharp_turn_left_boundary = round(image.shape[1] * 0.22)\n",
    "        sharp_turn_right_boundary = image.shape[1] - sharp_turn_left_boundary\n",
    "        \n",
    "        if left_out_contours:\n",
    "            if previous_direction != \"left\":\n",
    "                robot.stop()\n",
    "                time.sleep(0.05)\n",
    "            robot.left(0.8)\n",
    "            previous_direction = \"left\"\n",
    "            display_color.value = bgr8_to_jpeg(cv2.resize(image,(320,240)))\n",
    "            return\n",
    "        elif right_out_contours:\n",
    "            if previous_direction != \"right\":\n",
    "                robot.stop()\n",
    "                time.sleep(0.05)\n",
    "            robot.right(0.8)\n",
    "            previous_direction = \"right\"\n",
    "            display_color.value = bgr8_to_jpeg(cv2.resize(image,(320,240)))\n",
    "            return\n",
    "        if largest_contour_centroid[0] < sharp_turn_left_boundary:\n",
    "            if previous_direction != \"left\":\n",
    "                robot.stop()\n",
    "                time.sleep(0.05)\n",
    "            robot.left(0.8)\n",
    "            previous_direction = \"left\"\n",
    "            display_color.value = bgr8_to_jpeg(cv2.resize(image,(320,240)))\n",
    "            return\n",
    "        elif largest_contour_centroid[0] > sharp_turn_right_boundary:\n",
    "            if previous_direction != \"right\":\n",
    "                robot.stop()\n",
    "                time.sleep(0.05)\n",
    "            robot.right(0.8)\n",
    "            previous_direction = \"right\"\n",
    "            display_color.value = bgr8_to_jpeg(cv2.resize(image,(320,240)))\n",
    "            return\n",
    "        \n",
    "        shallow_turn_left_boundary = round(image.shape[1] * 0.45)\n",
    "        shallow_turn_right_boundary = image.shape[1] - shallow_turn_left_boundary\n",
    "        speed = (1-max(0.1, min(cv2.contourArea(largest_contour) / 25000, 0.6))) \n",
    "        \n",
    "        if largest_contour_centroid[0] < shallow_turn_left_boundary:\n",
    "            robot.forward_left(speed)\n",
    "            previous_direction = \"forward_left\"\n",
    "            display_color.value = bgr8_to_jpeg(cv2.resize(image,(320,240)))\n",
    "            return\n",
    "        elif largest_contour_centroid[0] > shallow_turn_right_boundary:\n",
    "            robot.forward_right(speed)\n",
    "            previous_direction = \"forward_right\"\n",
    "            display_color.value = bgr8_to_jpeg(cv2.resize(image,(320,240)))\n",
    "            return\n",
    "    else:\n",
    "        if previous_direction == \"left\":\n",
    "            if previous_direction != \"left\":\n",
    "                robot.stop()\n",
    "                time.sleep(0.05)\n",
    "            robot.left(0.8)\n",
    "            previous_direction = \"left\"\n",
    "        elif previous_direction == \"right\":\n",
    "            if previous_direction != \"right\":\n",
    "                robot.stop()\n",
    "                time.sleep(0.05)\n",
    "            robot.right(0.8)\n",
    "            previous_direction = \"right\"\n",
    "        else:\n",
    "            robot.backward(0.3)\n",
    "    \n",
    "    display_color.value = bgr8_to_jpeg(cv2.resize(image,(320,240)))\n",
    "\n",
    "#processing({'new': camera.color_value})\n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the processing function will be excuted.\n",
    "camera.observe(process, names='color_value')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
